---
title: "6+ Thing You Need to Know About Cluster Randomization"
author: "Methods Guide Author: Jake Bowers"
output: html_document
---

Abstract
==
This guide[^1] and all of the code it contains is available for copying at https://github.com/bowers-illinois-edu/EgapMethodsGuides. We encourage you to copy and improve the guide. See https://guides.github.com/activities/forking/ for one workflow in which you copy the guide, make your own changes, and then request that we include your changes in the main guide.

This guide involves a lot of R code R Core Team (2014) that we use to show how things work and to enable you to experiment and also to adapt it for your own particular purposes.

[^1]: Originating author: Jake Bowers and Ashlea Rundlett, 22 Nov 2014. The guide is a live document and subject to updating by EGAP members at any time. Bowers and Rundlett are not responsible for subsequent edits or errors introduced to this guide.

1 What it is, what it isn’t, and why we use it.
==
Cluster randomized experiments allocate treatments across groups of individuals as opposed to single individuals. Nonetheless, such studies still typically measure outcomes at the level of the individual. A study randomly assigning villages to receive different development programs but measuring individual level outcomes and a study randomly assigning households to receive different voter mobilization messages but measuring the vote turnout of individuals are both cluster randomized experiments: villages and households are the assignment units and individuals are the outcome units. A study which samples villages from the experimental pool, and then randomly assigns some of the people in each village to a treatment is not a cluster randomized experiment: in this study, both the units of assignment and outcome are the individual. A study which randomly assigned villages to an intervention and then measures village level responses is also not a cluster randomized study: it is a village level study, the units of assignment and outcome are the same.

Cluster randomization should not be confused with block randomization. If an experimenter believes that baseline outcomes vary based on pre-treatment covariates _and/or_ that treatment effects will differ in substantively important ways between subgroups (also based on pre-treatment covariates), she may divide the experimental pool into groups that are homogeneous on those covariates (like gender or village size) and randomize within those blocks — effectively turning one large experiment into multiple mini-experiments. Cluster randomization can, however, be combined with block randomization. For example, one can collect clusters of individuals (villages, classrooms, schools, households, etc.) into blocks based on cluster-level covariates and then run a blocked cluster randomized experiment in order to learn about subgroup differences in treatment effects and, at the same time, increase the precision of statistical tests. In this guide, we ignore such blocking in order to focus on cluster-lever or group-level treatment assignment with individual-level measurement.

Researchers might choose to utlize cluster randomization for a number of reasons. For example, a researcher may be interested in village/school/household-level interventions in and of themselves, possibly because they want to know the effect of every individual in a given cluster being assigned to the same treatment category. Often, however, treatments are randomly assigned at the cluster level because it is too expensive or even impossible to randomize at the individual level (radio signals, for example, either go to a geographical area or don't—you can't vary reception on an individual level).

2 Why Cluster Randomization can cause problems
==
Cluster randomized experiments have at least two units of analysis: We commonly see a few large assignment units ($J$), each containing some outcome units ($n_j$) and thus the total sample size depends on both assignment and outcome units $N=∑^J_{j=1}n_j$.

Cluster randomized experiments raise two new questions for analysts. The first question is about weighting, or how to combine information from different experimental clusters into one quantity. If clusters are not all the same size  (i.e. $n_j≠n_k$  for $j≠k$ ), then an average treatment effect must be defined in a weighted fashion and the resulting estimation should also involve weights. What weights should one use? On what basis should one choose weights? One component of weights should account for the size of the cluster (larger clusters tell us more about the treatment effect than smaller clusters, all other things equal). Another component would add that homogeneous clusters (where all villagers behave in the same way in response to treatment) tell us less about the treatment effect than heterogeneous clusters (where each villager acts as if she were more or less independent of the others). If the study is blocked, then an analyst need to choose a block-weighting scheme and cluster-weighting scheme. Hansen and Bowers (2008) discuss optimal weights for precise testing in blocked and cluster-randomized designs. Imai, King, and Nall (2009) discuss weighting schemes for estimation in paired and cluster-randomizied designs.

The second question is about information. We commonly summarize the information content of a study using the total number of participants, $N$. For example, we tend to imagine that a study with 10 people has less information about the experimental intervention than a study with 100 people. Yet, two studies with $J=10$  villages and $n_j=100$  people per village may have different information about the treatment effect on individuals if, in one study, individuals within a village are more or less independent of each other versus more or less dependent. If, say, all of the individuals in any village acted exactly the same but different villages showed different outcomes, then we would have on the order of 10 pieces of information: all of the information about causal effects in that study would be at the village level. Alternatively, if the individuals within a village acted more or less independently of each other, then we would have on the order of 10 × 100=1000 pieces of information. For a given variable, we can formalize the idea that the highly dependent clusters provide less information than the highly independent clusters with the intracluster correlation coefficient. For a given variable, $x$, we can write the intracluster correlation coefficient like so:

$$ρ ≡ \frac{\text{variance between clusters in x}}{\text{total variance in x}} ≡ \frac{τ^2_x}{τ^2_x+σ^2_x}$$

where $σ^2_x$ is the variance within clusters and $τ^2_x$ is the variance across clusters. For example, Kish (1965) uses this description of dependence to define his idea of the “effective N” of a study (in the sample survey context, where samples may be clustered):

$$\text{effective N} = \frac{N}{1+(n_j−1)ρ} = \frac{J_n}{1+(n−1)ρ}$$

where the second term follows if all of the clusters are the same size ($n_1=…=n_J≡n$).

If 1000 observations arose from 10 clusters with 20 individuals within each cluster where 50% of the variation could be attributed to cluster-to-cluster differences (and not to differences within a cluster), Kish’s formula would suggest that we have the equivalent of about 19 pieces of independent information not 10 × 20=200 pieces.

The inflation in the standard errors for estimators of the average treatment effect depends on $ρ$ as well. In this simple case with 10 clusters all the same size of 20 and $ρ=.5$, the standard error of the estimator accounting for $ρ$ is $1+(n−1)ρ=10.5$ times larger than the standard error that a simple t-test from a linear regression would provide: if $Var(\hat{\bar{Y}}_{z_{ij=1}})=\frac{s^2}{n}$ then accounting for clustered assignment with same size clusters would give us $Var(\hat{\bar{Y}}_{z_{ij=1}})=\frac{s}{(J_n)(1−(n−1)ρ)}$.[^2]

[^2]: See the following pieces for more discussion in general of the problems that arise from clustered designs in the study of politics L. Stoker and Bowers (2002), Laura Stoker and Bowers (2002), Green and Vavreck (2007), Arceneaux and Nickerson (2009)

The fact that most clustered designs contain less information than observations can lead to invalid statistical inferences. If, for example, the true standard error is ten times the estimated standard error, then our confidence intervals and statistical tests will be wildly invalid — we will be rejecting the null of no effects much too often. Without accounting for the design, we will be mislead by reports that we have ample information to reject a null of no effects: we will claim that a result is “statistically significant” when it is not.

3 Statistical inference for the average treatment effects in cluster randomized experiments part 1: Design-Based Approaches
==
## 3.1 What is a standard error?

How would an estimate of the average treatment effect vary if we repeated the experiment on the same group of villages? The standard error of an estimate of the average treatment effect is one answer to this question. Below, we simulate a simple, individual-level experiment to develop intuition about what a standard error is.[^3]

[^3]: See https://egap.org/resource/10-types-of-treatment-effect-you-should-know-about/ for a demonstration that the difference of means in the observed treatment and control groups is an unbiased estimator of the average treatment effect itself and what it means to be unbiased.

```{r}
N<-100
tau<-.25
y0<-rnorm(N) ## potential outcomes to control
y1<-y0+tau   ## potential outcomes to treatment are a simple function of y0
Zrealized<-sample(rep(c(0,1),N/2)) ## Assign treatment to half

fastmean<-function(x){
  ## A Fast mean calculator (see the mean.default function for all it does that we do not want to do)
  .Internal(mean(x))
}

fastvar<-function(x){
  ## See var for this. Right now it removes missing values
  .Call(stats:::C_cov,x,NULL,5,FALSE)
}

simEstAte<-function(Z,y1,y0){
  ## A function to re-assign treatment and recalculate the difference of means
  Znew<-sample(Z)
  Y<-Znew*y1+(1-Znew)*y0
  estate<-fastmean(Y[Znew==1])-fastmean(Y[Znew==0])
  return(estate)
}

set.seed(12345)
simpleResults<-replicate(100000,simEstAte(Z=Zrealized,y1=y1,y0=y0))
sd(simpleResults) ## The standard error of the estimate of the ATE.
```

Although this preceding standard error is intuitive (it is merely the standard deviation of the distribution arising from repeating the experiment), more statistics-savvy readers will recognize closed-form expressions for the standard error like the following (see Gerber and Green (2012) and Dunning (2012) for easy to read explanations and derivations of the design-based standard error of the simple estimator of the average treatment effect). If we write T as the set of all treated units and C as the set of all non treated units, we might write

$$\widehat{Var}(\hat{τ}) = s^2(Y_{i,i_∈T})/m+s^2(Y_{i,i∈C}/(n−m))$$

where $m$ is the number assigned to treatment and $s^2(x)=(1/n−1)∑^n_{i=1}(x_i–\bar{x})^2$. Here we compare the results of the simulation to this most common standard error as well as to the “true” version (which requires that we know the potential outcomes so as to calculate their covariance):

```{r, error=FALSE, warning=FALSE, message=FALSE}
## True SE (Dunning Chap 6, Gerber and Green Chap 3 and Freedman, Pisani and Purves A-32) including the covariance between the potential outcomes

V<-var(cbind(y0,y1))
varc<-V[1,1]
vart<-V[2,2]
covtc<-V[1,2]
n<-sum(Zrealized)
m<-N-n

varestATE<-((N-n)/(N-1))*(vart/n) + ((N-m)/(N-1))* (varc/m) + (2/(N-1)) * covtc
seEstATE<-sqrt(varestATE)

## And the finite sample *feasible* version (where we do not observe the potential outcomes) and so we do not have the covariance
Yobs<-Zrealized*y1+(1-Zrealized)*y0
varYc<-var(Yobs[Zrealized==0])
varYt<-var(Yobs[Zrealized==1])
fvarestATE<-(N/(N-1)) * ( (varYt/n) + (varYc/m) )
estSEEstATE<-sqrt(fvarestATE)

##  Here we use the HC2 standard error --- which Lin 2013 shows is the randomization justied SE for OLS.
library(sandwich)
library(lmtest)

lm1<-lm(Yobs~Zrealized)

## Other SEs
iidSE<- sqrt(diag(vcov(lm1)))[["Zrealized"]]

## Worth noting that if we had covariates in the model we would want this one (which is identical to the previous one without covariates).
NeymanSE<-sqrt(diag(vcovHC(lm1,type="HC2")))[["Zrealized"]]

c(simSE=sd(simpleResults),feasibleSE=estSEEstATE, trueSE=seEstATE, olsIIDSE=iidSE, NeymanDesignSE=NeymanSE)
```

We see that the feasible SE (also known as the conservative SE) and the true SE are the same to 3 digits here, where as the OLS versions are a bit smaller and the simulated standard error is also very close. These standard errors will diverge when covariates are introduced into the linear model. And, of course, the true version is rarely calculable since we don't have access to the true potential outcomes.

##3.2 Standard Errors reflecting cluster-assignment of treatment

To begin, we will create a function which simulates a cluster randomized experiment with fixed intracluster correlation.[^4]

[^4]: Code available on the github repository shows that the ICC will increase as an additive cluster-level treatment effect increases. See Mathieu et al. (2012a) and Mathieu et al. (2012b) for some code that inspired the code we use here.

```{r}
ClusteredData<-function(J,n,tau,rho){
  ## Inspired by Mathieu et al, 2012, Journal of Applied Psychology
  if (J %% 2 != 0 | n %% 2 !=0) {
    stop(paste("Number of clusters (J) and size of clusters (n) must be even."))
  }

  ## If we do not inflate the variation in the baseline potential outcome (y0)
  ## by a function of tau (cluster level additive effect), then as tau increases,
  ## the ICC will necessarily increase.

  #y0j<-rnorm(J,0,sd=sqrt(rho))
  y0j<-rnorm(J,0,sd=(1+tau)^2 * sqrt(rho))
  dat<-expand.grid(i=1:n,J=1:J)
  #dat$y0 <- rnorm(n*J,0,sd=sqrt(1-rho))+y0j[dat$J]
  dat$y0 <- rnorm(n*J,0,sd=(1+tau)^2 * sqrt(1-rho))+y0j[dat$J]
  dat$y1 <- with(dat,fastmean(y0)+tau+(y0-fastmean(y0))*(2/3)) ## give treated group mean shift of tau but also smaller variance
  dat$Zi <- ifelse(dat$J %in% sample(1:J,J/2) == TRUE, 1, 0)
  dat$Y  <- with(dat, Zi*y1 + (1-Zi)*y0)
  return(dat)
}
```

Simulate some data from a simple cluster-randomized design for other demonstrations.

```{r}
set.seed(12345)
pretendDat<-ClusteredData(J=100,n=10,tau=.25,rho=.1)
```

##3.3 Cluster-Level Analysis

When the assignment units are clusters and the analysis units are individuals, the standard error refers to repeated reassignments of treatment to clusters. One way to avoid the problem of changing the way that standard errors are calculated is to analyze the data at the level of the cluster — that is, to take averages or sums of the outcomes within the clusters, and then to treat the study as a single-level study (with weights if the clusters vary in size) (See Dunning (2012) for an argument in favor of this approach).

Hansen and Bowers (2008) also recommend a variant of this approach which we demonstrate here. They write a test statistic as a cluster-weighted difference of means: $d(z,y)=z^Ty/z^Tm−(1−z)^Ty/(1−z)^Tm$ where $m$ is a $J×1$ vector recording the size of each cluster, and $z$ and $y$ are also both $J×1$ vectors recording the treatment assignment and observed outcome of each cluster respectively. They then show that one can write a difference of means as a shifted sum, $d(z,y)=z^Ty∗(n/(m_0n_t(n–n_t)))–1^Ty/(m_0(n–n_t))$ where $m_0$ is the size of a cluster, $n$ is the size of the experimental pool of clusters, and $n_t$ is the number of treated clusters (using their notation to make study of that paper easier) and thus, that we can characterize the distribution of the difference of means using what we know about the distribution of the only random piece of this expression, the sum of the outcome in the treatment group — $z^Ty$. We demonstrate this approach here:

```{r}
## Make a data frame at the cluster level
clusterDat<-data.frame(Yj=tapply(pretendDat$Y,pretendDat$J,sum),
               Zj=tapply(pretendDat$Zi,pretendDat$J,unique))
row.names(clusterDat)<-attr(clusterDat$Zj,"dimnames")[[1]]
clustersize<-table(pretendDat$J)
clusterDat[names(clustersize),"mj"]<-clustersize
clusterDat$ids<-as.numeric(row.names(clusterDat))

## Three equivalent formulas following Hansen and Bowers 2008 for the difference of means given clustered treatment assignment (and equal sized clusters and no blocking).

dp1<-function(x,z,m){
  crossprod(z,x)/crossprod(z,m) - crossprod(1-z,x)/crossprod(1-z,m)
}

dp2<-function(x,z,m0){
  ## m0 is a scalar cluster size
  nt<-sum(z)
  nc<-sum(1-z)
  k0<-m0*nt
  k1<-m0*nc
  crossprod(z,x)/k0 - crossprod((1-z),x)/k1
}

dp3<-function(x,z,m0,nt=sum(z),n=length(z)){
  ## m0 is a scalar cluster size
  ones<-rep(1,length(x))
  crossprod(z,x)*(n/(m0*nt*(n-nt))) - crossprod(ones,x)/(m0*(n-nt))
}

## Given equal sized clusters and no blocking, this is just the difference of means:


c(dp1= with(clusterDat,dp1(Yj,Zj,mj)),
  dp2= with(clusterDat,dp2(Yj,Zj,unique(mj))),
  dp3= with(clusterDat,dp3(Yj,Zj,unique(mj))),
  meandiff = with(pretendDat,mean(Y[Zi==1])-mean(Y[Zi==0])) )
```

Here we use the Hansen and Bowers (2008) approach to calculate the standard error of the difference in means as an estimator of the average treatment effect.

```{r}
## Now we use the Hansen and Bowers 2008 approach for the variance of the cluster-level diff of means (recall var(a*x)=a^*var(x)
# Notice that we have the cluster-size in the denominator where, in the simple calc of the design-based variance of the total of Y in the treatment group we do not refer to cluster size
# We are still assuming equal cluster sizes.

## Because the dp3 representation has only one random term crossprod(z,x), we can approximate the design based variance of this test statistic dp() using the variance(crossprod(z,x)*constant)

n<-nrow(clusterDat) ## number of clusters
nt<-sum(clusterDat$Zj) ## fixed number of treatment assigned clusters

## This from Lohr on Variance of sample total.
##Vtot <- with(clusterDat,(n^2) * (1-(nt/n)) * ( var(Yj[Zj==1])/nt ))
##sqrt(Vtot)

## Testing our code
## library(survey)
## des<-svydesign(ids=~ids,data=clusterDat[clusterDat$Zj==1,],fpc=rep(.5,n/2))
##thetot<-svytotal(~Yj,design=des)
##thetot

m0<-unique(clusterDat$mj) ## assuming all clusters same size
Vdp<-(n/(m0*nt*(n-nt)))*(var(clusterDat$Yj)/m0)
sqrt(Vdp)

## We can evaluate the hypothesis of no effects with this Normal approximation:

## First show that our analytics are on target
set.seed(12345)
thedist<-with(clusterDat,replicate(10000,dp3(Yj,sample(Zj),m0)))
sd(thedist)

plot(density(thedist))
curve(dnorm(x,mean=0,sd=sqrt(Vdp)),col="red",add=TRUE)

obsmeandiff<-with(clusterDat,dp3(Yj,Zj,m0))[1,1]

pSim<-2*min( mean(thedist>=obsmeandiff),mean(thedist>=obsmeandiff))
##pH0<-2*min(pnorm(c(-1,1)*obsmeandiff,mean=0,sd=sqrt(Vdp)))
pH0<-2*(1-pnorm(abs(obsmeandiff)/sqrt(Vdp),mean=0))

c(SimulatedP=pSim,HansenBowersP=pH0)
```

And here we combine the preceding information into a general use unfunction for testing the null of no effects in large samples using the difference of means.

```{r}
hbtest<-function(x,z,m0,n=length(z),nt=sum(z)){
  ## Hansen and Bowers 2008 based test for diff of means with cluster level assignment
  ## assuming same size clusters. See also that article for other caveats about the Normal
  ## approximation used here.
  obsmeandiff<-dp3(x=x,z=z,m0=m0,nt=nt,n=n)[1,1]
  ## Returns a two tailed p-value for the test of the null of no effects
  Vdp<-(n/(m0*nt*(n-nt)))*(fastvar(x)/m0)
  ## tailp<-pnorm(obsmeandiff/sqrt(Vdp))
  ## 2*min(c(tailp,1-tailp))
  return( 2*(1-pnorm(abs(obsmeandiff)/sqrt(Vdp))) )
}

with(clusterDat,hbtest(x=Yj,z=Zj,m0=m0))
```

##3.4 Individual Level Analysis accounting for clustering

Alternatively, one could adjust the IID standard error for clustering. If all clusters are the same size, $n_1=…=n_J=n$, and the experiment has no blocking, the formula that summarizes this repetition is:

$$\text{Var}_\text{clustered}(\hat{\tau})=\frac{\sigma^2}{\sum_{j=1}^J \sum_{i=1}^n_j (Z_{ij}-\bar{Z})^2} (1-(n-1)\rho)$$

where $\sigma^2=\sum_{j=1}^J \sum_{i=1}^n_j (Y_{ij} – \bar{Y}_{ij})^2$ (following Arceneaux and Nickerson (2009)). 

This adjustment is commonly known as the “Robust Clustered Standard Error” or RCSE. Here we demonstrate how the RCSE relates to the standard error calculated by (1) ignoring the clustering and (2) repeating the experiment using known potential outcomes.

If we repeat the experiment we can see the variation in the treatment effect calculated at the individual level induced by re-assignment of treatment to clusters.

```{r}
rcse <- function(model, cluster){
  ## clustered standard errors from http://drewdimmery.com/robust-ses-in-r/
  ##require(sandwich)
  ##require(lmtest)
  M <- length(unique(cluster))
  N <- length(cluster)
  K <- model$rank
  dfc <- (M/(M - 1)) * ((N - 1)/(N - K))
  uj <- apply(estfun(model), 2, function(x) tapply(x, cluster, sum));
  rcse.cov <- dfc * sandwich(model, meat = crossprod(uj)/N)
  rcse.se <- coeftest(model, rcse.cov)
  return(list(rcse.cov, rcse.se))
}

## Yet another way to calculate a mean difference:
lm2<-lm(Y~Zi,data=pretendDat)
lm2rcse<-rcse(lm2,pretendDat$J)
iidSE2<- sqrt(diag(vcov(lm2)))[["Zi"]]
rcSE2<-sqrt(diag(lm2rcse[[1]]))[["Zi"]]


## Now benchmark these analytic results against what we would see if we knew the potential outcomes and could repeat the design.
simEstAteClustered<-function(J,y1,y0){
  ## Equal sized clusters and equal numbers of treated and control clusters
  z.sim <- sample(1:max(J), max(J)/2)
  Znew <- ifelse(J %in% z.sim == TRUE, 1, 0)
  Y<-Znew*y1+(1-Znew)*y0
  estate<-fastmean(Y[Znew==1])-fastmean(Y[Znew==0])
  return(estate)
}

set.seed(12345)
clusterResults<-replicate(10000,
              simEstAteClustered(J=pretendDat$J,
                         y1=pretendDat$y1,
                         y0=pretendDat$y0))


c(simClusterSE=sd(clusterResults),
  rcSE=rcSE2,
  iidSE2=iidSE2)
```

When we ignore the clustered-assignment, the standard error is small. The RCSE and the directly simulated version agree, within simulation error, to three digits. Now, one question is whether the RCSE is a good approximation directly repeating the experiment if the experiment is small. (We set aside questions of whether the coverage of confidence intervals for the next section.)

Here we show that the RCSE is sensitive to the number of clusters.

```{r}
## And now showing that the simulated and rcse results become the same
## once the number of clusters is large (revealing that the rcse is consistent in the number of clusters).
##  require(ICC)
compareClusterSEs<-function(J,n,tau){
  set.seed(12345)
  pretendDat3<-ClusteredData(J,n,tau,.1)
  ##theicc<-ICCbare(y=Y,x=J,data=pretendDat3)
  set.seed(12345)
  clusterResults3<-replicate(10000,
                 simEstAteClustered(J=pretendDat3$J,
                        y1=pretendDat3$y1,
                        y0=pretendDat3$y0))
  lm3<-lm(Y~Zi,data=pretendDat3)
  lm3rcse<-rcse(lm3,pretendDat3$J)
  iidSE3<- sqrt(diag(vcov(lm3)))[["Zi"]]
  rcSE3<-sqrt(diag(lm3rcse[[1]]))[["Zi"]]
  return(c(##ICC=theicc[[1]],
       simClusterSE=sd(clusterResults3),
       rcSE=rcSE3,
       iidSE2=iidSE3))
}

compareClusterSEs(4,10,.25)

compareClusterSEs(30,10,.25)

compareClusterSEs(100,10,.25)

compareClusterSEs(1000,10,.25)
```

Standard errors accounting for clustering (the `rcSE`) and the simulation based standard error (`simClusterSE`) come to resemble one another more and more as the number of clusters increases. Meanwhile, the standard error ignoring clustering tends to be smaller than either of the other standard errors. This raises the question about when tests and confidence intervals based on these standard errors will perform well and when they will perform poorly: the fact that some standard errors are smaller than others does not tell us that we should avoid the small ones. Thus, in the next section we assess the false positive rate of tests based on these standard errors.

Notice that by holding cluster size fixed, we can assume that the size of the cluster is unrelated to the potential outcomes. If cluster sizes do vary then Middleton (2008) and Middleton and Aronow (2011) teach us about how to adjust standard errors and estimates of the ATE to avoid bias. See also Small, Ten Have, and Rosenbaum (2008) and Hansen and Bowers (2009) for flexible approaches to design-based tests of causal effects in experiments with cluster-assignment and non-compliance, unequal sized clusters, and covariates used to increase precision.

##3.5 Design-based performance of different methods of accounting for clustered assignment.

Here we show that the false positive rate of tests based on RCSE standard errors tends to be incorrect when the number of clusters is small. The individual-level plus cluster-correction approach becomes valid only when the number of clusters is large.[^5]

[^5]: A valid test is one with an error rate less than or equal to the declared level of the test – here pegged at α=.05. Valid tests may be overly conservative, but not overly liberal.

```{r}
confint.HC<-function (b, df, level = 0.95, thevcov, ...) {
  ## CI for lm with custom vcov 
  ## a stripped down copy of the confint.lm function adding "thevcov" argument
  a <- (1 - level)/2
  a <- c(a, 1 - a)
  fac <- qt(a, df)
  ses <- sqrt(diag(thevcov))
  b + ses %o% fac
}

simAteZeroClustered<-function(J,Y){
  ## Make the true relationship equal zero by shuffling Z but not revealing new potential outcomes
  z.sim <- sample(1:max(J), max(J)/2)
  Znew <- ifelse(J %in% z.sim == TRUE, 1, 0)
  lm1<-lm(Y~Znew)
  lm1rcse<-rcse(lm1,J)
  lm1ci<-confint.HC(b=coef(lm1), thevcov=lm1rcse[[1]], lm1$df.residual)["Znew",]
  ### is zero in the CI?
  zeroInCIlm<- 0 >= lm1ci[1] & 0 <= lm1ci[2]
  ## Make cluster level data following Hansen and Bowers 2008
  Yj<-tapply(Y,J,sum)
  Zj<-tapply(Znew,J,fastmean)
  m0<-unique(table(J))
  ## Do hbtest
  zeroNotRej<-hbtest(x=Yj,z=Zj,m0=m0)>=.05
  return(c(estate=coef(lm1)["Znew"],zeroInCIlm=zeroInCIlm,zeroNotRej=zeroNotRej))
}

set.seed(12345)
pretendDatJ4<-ClusteredData(4,10,.25,.1)
set.seed(12345)
pretendDatJ10<-ClusteredData(10,10,.25,.1)
set.seed(12345)
pretendDatJ30<-ClusteredData(30,10,.25,.1)
set.seed(12345)
pretendDatJ100<-ClusteredData(100,10,.25,.1)

J4res<-replicate(10000, simAteZeroClustered(J=pretendDatJ4$J, Y=pretendDatJ4$Y))
J4error<-apply(J4res,1,mean)[2:3]

J10res<-replicate(10000, simAteZeroClustered(J=pretendDatJ10$J, Y=pretendDatJ10$Y))
J10error<-apply(J10res,1,mean)[2:3]

J30res<-replicate(10000, simAteZeroClustered(J=pretendDatJ30$J, Y=pretendDatJ30$Y))
J30error<-apply(J30res,1,mean)[2:3]

J100res<-replicate(10000, simAteZeroClustered(J=pretendDatJ100$J, Y=pretendDatJ100$Y))
J100error<-apply(J100res,1,mean)[2:3]
desmat<-rbind(J4error,
      J10error,
      J30error,
      J100error)
colnames(desmat)<-c("OLS+RCSE","Cluster-level")
print(desmat)
```

When the number of clusters is very small ($J=4$) the cluster-level approach is conservative but the individual-level approach is overly liberal: the 95% CI would exclude the truth 100-66=34% of the time rather than only 100-95=5% of the time (or 0% of the time in the case of the cluster-level approach). As the number of clusters increases, the performance of both design-based statistical inference procedures improves although the RCSE approach continues to be liberal compared to the cluster-level approach.

4 Statistical inference for the average treatment effect in cluster randomized experiments part 2: Model-Based Approaches
==
##4.1 Why call an approach Model-Based versus Design-Based?

We have shown two approaches to statistical inference about the average treatment effect which require that (1) that treatment was randomized as planned and (2) that the treatment assigned to one unit did not change the potential outcomes for any other unit. To clarify concepts and code we also assumed no blocking, no covariates (which could be used to increase precision), continuous outcomes, no missing outcomes, and equal sized clusters. We also focused on the “intent to treat” effect. The design-based approach can be extended to handle more complex designs.[^6] For a nice example of that approach see the the [EGAP Power Calculator](https://egap.shinyapps.io/Power_Calculator) for Simple and Clustered Designs which uses the design-based approach from Hayes and Moulton, 2009 Chap 7.

[^6]: For example, Hansen and Bowers (2009) analyze a cluster-randomized field experiment with one-sided non-compliance and a binary outcome using a design-based approach. And Small, Ten Have, and Rosenbaum (2008) show how to handle more complex hypotheses about effects in cluster-randomized trial with a small number of clusters, covariance adjustment, and non-compliance.

When designs are more complicated, deriving the expressions for standard errors can become difficult. An alternative approach is to directly model both the outcome (often, when continuous outcomes, as a Normal random variable) and the cluster-to-cluster differences in the outcome in the control group (often, as a Normal random variable, too) using a multilevel model like the following. We can write this model in matrix form as follows with $Y$, the vector of observed outcomes which depends on individual level covariates in the matrix, and $X$, a variance-covariance matrix representing relationships among individuals within clusters of $Σ^Y$, and another model of the cluster-to-cluster variation in the coefficients $β$ (which, here, depends on cluster-level variables and another variance-covariance matrix describing relations among clusters). Raudenbush (1997) and Raudenbush and Liu (2000), Hong and Raudenbush (2006) and Raudenbush, Martinez, and Spybrook (2007) all grapple with issues of design and causal inference under model-based approaches to the study of cluster-randomized trials in the context of education.

$$Y|X,\beta,\Sigma_Y \sim N(X\beta,\Sigma_Y)$$
$$\beta|Z,\gamma,\Sigma_\beta \sim N(Z\gamma,\Sigma_\beta)$$

We can write a piece of this model in scalar form to show that, in our simple designs here, $X=1$, $β$ only refers to the cluster-specific intercept, and $\gamma$ refers to the overall intercept and treatment effect estimate.

$$Y_{ij}=β_{0j}+ε_{ij}$$
$$β_{0j}=γ_{00}+γ_{01}Z_j+ν_{0j}$$

These kinds of Bayesian-inspired models imply a particular kind of weighting across clusters and blocks which arises as a by-product of Bayes Rule.[^7] In particular, in this Normal-likelihood and Normal-cluster-prior model, the weights take the following form, where, as we can see below, the distribution of the $β$ depends on the different relationships at both the individual and cluster-level.

[^7]: Gelman and Hill (2007) for an introduction to multilevel models, including in the context of statistical inference for counterfactual causal effects, and Gelman et al. (2013) (Chap 8) for an account linking Bayesian approaches to statistical inference to design and especially randomization.

$$y|X,\beta,\Sigma_Y,\gamma,Z,\Sigma_\beta \sim N(XZ_\gamma,\Sigma_Y+X\Sigma_{\beta}X^\tau)$$
$$\beta|y,Z,\gamma,\Sigma_{\beta},\Sigma_{\gamma},\Sigma_Y \sim N\left( (X^{\tau}\Sigma^{-1}_YX+\Sigma^{-1}_{\beta})^{-1}(X^{\tau}\Sigma^{-1}_{\gamma}y+\Sigma^{-1}_{\beta}Z_{\gamma}),(X^{\tau}\Sigma^{-1}_YX+\Sigma^{-1}_{\beta})^{-1} \right)$$

Notice that the flavor of these expressions differs from those we discussed above. In the design-based approach we referred to repetitions of the experiment to derive and check expressions for the variance of estimates that accounted for cluster-level assignment. In the model-based approach we state that the outcomes were generated according to a probability model (here a Normal model) and that the cluster-level relationships also follow a probability model (here, also a Normal model). It is sometimes simpler to state such models — even if we do not believe the models as scientific descriptions of a known process — and then assess their characteristics (say, their error rates and power) than it is to derive new expressions for design-based estimators when designs are complex. Furthermore, in some situations where clusters (like schools) are many and are a random sample of a population of such clusters, the Normal models may describe the outcomes and cluster-to-cluster variation process well.[^8]

[^8]: See Barnard et al. (2003) for an example of an application of this approach when we have other problems such as missing outcomes, or complex non-compliance.

##4.2 How can we estimate model-based estimates of the ATE?

Here we show that the estimated effect is the same whether we use a simple difference of means (via OLS) or a multilevel model in our very simplified cluster randomized trial setup.

```{r, warning=FALSE, error=FALSE, message=FALSE}
library(lme4)

lm4<-lm(Y~Zi,data=pretendDatJ30)
lmer1<-lmer(Y~Zi+(1|J),data=pretendDatJ30, control=lmerControl(optimizer='bobyqa'),REML=TRUE)

c(OLS=coef(lm4)["Zi"],Multilevel=fixef(lmer1)["Zi"])
```

The confidence intervals differ even though the estimates as the same — and there is more than one way to calculate confidence intervals and hypothesis tests for multilevel models. The software in R (Bates, Maechler, Bolker, et al. (2014a), Bates, Maechler, Bolker, et al. (2014b)) includes three methods by default and Gelman and Hill (2007) recommend MCMC sampling from the implied posterior. Here we focus on the Wald method only because it is the fastest to compute. Other methods might show other performance when we evaluate these methods below.

```{r, message=FALSE, error=FALSE, warning=FALSE}
lm4ci<-confint.HC(b=coef(lm4), thevcov=rcse(lm4,pretendDatJ30$J)[[1]], lm4$df.residual)["Zi",]
lmer1ciWald<-lme4:::confint.merMod(lmer1,parm="Zi",method="Wald")["Zi",]
lmer1ciProfile<-lme4:::confint.merMod(lmer1,parm=4,method="profile")["Zi",]

rbind(DesignBasedCI=lm4ci,
      ModelBasedWaldCI=lmer1ciWald,
      ModelBasedProfileCI=lmer1ciProfile)
```

We can calculate an estimate of the ICC directly from the model quantities (the variance of the Normal prior that represents the cluster-to-cluster differences in the intercept over the total variance of the Normal posterior).

```{r, message=FALSE, error=FALSE, warning=FALSE}
VC<-as.data.frame(lme4:::VarCorr.merMod(lmer1))
VC$vcov[1]/sum(VC$vcov)
```

##4.3 How do model-based approaches perform? Validity

We showed that the large-sample/Normal theory design-based statistical inference did not produce valid confidence intervals and statistical tests until at least 30 if not more clusters. Here we compare the design-based approach to the model-based approach (using the Wald method for calculating confidence intervals). Although the outcome-model based approach is Bayesian in structure, we can evaluate its properties across repetitions of the design. We do this here.

```{r}
simAteZeroLmer<-function(J,Y){
  ## Make the true relationship equal zero by shuffling Z but not revealing new potential outcomes
  z.sim <- sample(1:max(J), max(J)/2)
  Znew <- ifelse(J %in% z.sim == TRUE, 1, 0)
  thelmer<-lmer(Y~Znew+(1|J), control=lmerControl(optimizer='bobyqa'),REML=FALSE)
  lmer1ciWald<-lme4:::confint.merMod(thelmer,parm="Znew",method="Wald")
  ## lmer1ciProfile<-lme4:::confint.merMod(thelmer,parm=4,method="profile")
  ### is zero in the CI?
  zeroInCIWald <- 0 >= lmer1ciWald[1] & 0 <= lmer1ciWald[2]
  ## zeroInCIProfile <- 0 >= lmer1ciProfile[1] & 0 <= lmer1ciProfile[2]
  return(c(estate=fixef(thelmer)["Znew"],
       zeroInCIWald=zeroInCIWald #, zeroInCIProfile=zeroInCIProfile
       ))
}


J4resLmer<-replicate(1000, simAteZeroLmer(J=pretendDatJ4$J, Y=pretendDatJ4$Y))
J4errorMLM<-apply(J4resLmer,1,mean)

J10res<-replicate(1000, simAteZeroLmer(J=pretendDatJ10$J, Y=pretendDatJ10$Y))
J10errorMLM<-apply(J10res,1,mean)

J30res<-replicate(1000, simAteZeroLmer(J=pretendDatJ30$J, Y=pretendDatJ30$Y))
J30errorMLM<-apply(J30res,1,mean)

J100res<-replicate(1000, simAteZeroLmer(J=pretendDatJ100$J, Y=pretendDatJ100$Y))
J100errorMLM<-apply(J100res,1,mean)
```

Here are the error rates for the three approaches to statistical inference in cluster-randomized experiments as the number of clusters increases in our simple example study. Recall that valid tests would have error rates within 2 simulation standard errors of .95 — this would mean that a correct null hypothesis would be rejected no more than 5% of the time.

```{r, error=FALSE, message=FALSE, warning=FALSE}
twosimse<-2*sqrt( .05*.95/1000)
print(twosimse)

mat<-cbind("Multilevel"= c(J4errorMLM[2], J10errorMLM[2],
         J30errorMLM[2], J100errorMLM[2]),
       desmat
       )
row.names(mat)<-paste("J=",c(4,10,30,100),sep="")
mat
```

In our simple setup, the individual-level approaches behave about the same way: neither the design-based nor the model-based approach produces valid statistical inferences until the number of clusters is at least 30. This makes sense: both approaches rely on central limit theorems so that a Normal law can describe the distribution of the test statistic under the null hypothesis. The cluster-level approach is always valid, but sometimes produces overly large confidence intervals (when the number of clusters is small). When the number of clusters is large (say, 100), then all approaches are equivalent in terms of their error rates. Designs with few clusters should consider either the cluster-level approach using the normal approximation shown here or even direct permutation based approaches to statistical inference. Of course, the code in this guide can be used to assess concerns about test validity at different numbers of clusters.

5 Designing powerful cluster randomized studies: more small clusters are better than fewer larger ones
==
We want designs which enable statistical tests to strongly discount hypotheses consistent with the data rarely and reject hypotheses inconsistent with the data often. Textbooks often say this by referring to the desire to reject false hypothesis often (a desire for powerful tests) and to not-reject true hypotheses rarely (a desire for valid tests). We’ve seen that clustered designs with few clusters can cause statistical tests to reject data-consistent hypotheses too frequently. That is, the assumptions required for the validity of common tests (typically, large numbers of observations, or large quantities of information in general) are challenged by clustered designs, and the tests which account for clustering can be invalid if the number of clusters is small (or information is low at the cluster level in general). We’ve also seen that we can produce valid statistical tests for hypotheses about the average treatment effect using either Robust Clustered Standard Errors (RCSE) or Multilevel models or using the cluster-level approach described by Hansen and Bowers (2008). Valid tests are a precondition for thinking about powerful tests. Since we know how to produce valid statistical inferences, we can then ask whether we can enhance our ability to detect average treatment effects (to distinguish them from zero) using design.

The most important rule regarding the statistical power of clustered designs is that more small clusters are better than fewer larger ones. We will demonstrate this here. However, for some intuition consider again the formula for the effective sample size with equal sized clusters and no blocking.

$$\text{effective N} = \frac{J_n}{1+(n−1)ρ}$$

Note that the size of each cluster $n$ is present in both the numerator and denominator, the number of clusters $J$ is only present in the numerator. The size of each cluster, then, will do very little to increase our power unless the within-cluster dependence ($ρ$) is very small.

Here we demonstrate two approaches to calculating the power of cluster-randomized research designs: a fast approach based on closed-form expressions for the power of tests from multilevel models and a slow approach based on simulation. A closed-form approach based on the expressions from the RCSE and the cluster-level approaches would also be possible. We do not provide those expressions for now, since the Multilevel approach appears both more complex in terms of deriving power formulas and roughly equivalent to the RCSE approach in terms of validity based on our previous simulations presented above.

##5.1 Analytic Model-Based Power Analysis

Raudenbush and Liu (2000) and Raudenbush (1997) provide an expression for the power of a multilevel-model based test that enables quick evaluation of different multilevel design proposals defined by expected intracluster correlation ($ρ$), the fixed false positive rate ($α$), the effect size (Cohen’s $d$)(which is a standardized measure of the treatment effect that is common in psychology and educational research — it is roughly twice the standardized difference of means or standardized OLS $β$), the number of outcome units ($n$), and the number of assignment units ($J$). See Spybrook et al. (2011) for detailed explanations of these expressions.

```{r, message=FALSE, warning=FALSE, error=FALSE}
crtpow2<-function(alpha, d, rho, n, J){
  # Raudenbush (1997) power function see for details: http://hlmsoft.net/od/od-manual-20111016-v300.pdf
  ## rho=intraclass correlation [i.e. homogeneity within level-2 units]
  ## alpha=alpha level (here, power=power to detect an effect size of d at a=.05-->95% confidence level)
  ## d=effect size [note: d=.2 is like standardized OLS beta=.1, see Snijders and Bosker(1999), page 147 and below]
  ## n=number of level-1 units per level-2 unit
  ## J=number of level-2 units
  cpow <- 1 - pt(q=qt(1 - alpha, J -2), df=J - 2, ncp=d/(sqrt((4 * (rho + (1 - rho)/ n))/J)))
  cpow
}
```

This function uses quantities that we have already discussed. However, the effect size is defined as the difference of means standardized by the sample standard deviation of the outcome. Here we explore how this effect size relates to other quantities in order to provide some intuition to guide use of the formula:

```{r, error=FALSE, message=FALSE, warning=FALSE}
## About effect sizes measured as Cohen's d. How do they relate to a standardized mean difference?
## The unstandardized effect is the simple difference of means:
lmtemp<-lm(Y~Zi,data=pretendDat)
coef(lmtemp)["Zi"]

## Simple estATE
with(pretendDat,{ mean(Y[Zi==1]) - mean(Y[Zi==0]) })

## Here are two ways to think of the standardized effect size used in crtpow2()
## Cohen's d is this standardized effect
my.pooled.sd<-function(x,z){
  ## x is the numeric variable
  ## z is binary treatment
  ### the.vars <- tapply(x, z, var, na.rm = TRUE)
  ### the.pooled.sd <- sqrt(sum(the.vars)/2)
  ## this next is faster and taken from pairwise.t.test
  s <- tapply(x, z, sd, na.rm = TRUE)
  nobs <- tapply(!is.na(x), z, sum)
  degf <- nobs - 1
  total.degf <- sum(degf)
  return(sqrt(sum(s^2 * degf)/total.degf))
 }

thed<-with(pretendDat,{ ( mean(Y[Zi==1])-mean(Y[Zi==0]) ) / my.pooled.sd(Y,Zi) })
thed

## or following  Section 7 in Spybrook et al 2011
with(pretendDat,{ ( mean(Y[Zi==1])-mean(Y[Zi==0]) ) / sd(Y) })

## This standardized effect is different from the standardized regression coefficient in which treatment itself is standardized
## Standardized diff of means (regression beta)
lm.beta<-function(obj,term){
  ## from QuantPsyc
  b <- coef(obj)[term]
  sx <- sd(obj$model[[term]])
  sy <- sd(model.response(obj$model))
  beta <- b * sx/sy
  return(beta)
}

thetau<-lm.beta(lmtemp,"Zi")
thetau

## Another version of the standardized regression coef:
coef(lm(scale(Y)~scale(Zi),data=pretendDat))[2]

## So, a d of about .128 is a standardized reg coef of about .064
(thed/thetau)[[1]]

## So to use this function thinking in terms of standardized regression coefficients, one would double the desired coefficient.
## crtpow2(alpha=.05,d=.5,rho=0.3,n=10,J=50)
```

Now we will watch how power changes as we trade $n$ for $J$ and vary $ρ$ for $d=.3≈τ=.15$.

The figure below applies Raudenbush’s formula to show (1) the effect on power of increasing the number of clusters holding cluster size constant and large (at $n=50$) in the left panel versus (2) increasing cluster size while holding the number of clusters constant (at $J=50$) in the right panel. The left panel shows that power increases as the number of clusters increase but that this increase depends on the homogeneity of the clusters: very heterogeneous clusters would allow 80% power at roughly 20 clusters (assuming all tests are valid), but very homogeneous clusters require many more clusters to achieve the same power. The right panel shows that increasing the number of observations per cluster has little effect on power even when we allow clusters to become very large unless $ρ$ is small — and even then, there is an upper bound on the power from a study with 50 clusters.

![](https://raw.githubusercontent.com/egap/methods-guides/master/cluster-random/unnamed-chunk-9-1.png)

##5.2 Simulations for Power Analysis

The expressions we used above allow researchers to quickly assess many different values of $n$, $J$, $ρ$ and effect size. However, they are limited to certain designs (designs with equal numbers of treated and control units, designs with the same size clusters, with only two treatments, etc…) A simulation based approach allows more flexibility in the designs assessed at the price of computing time. We demonstrate a power analysis of this kind here.

The general idea should be familiar: first create a dataset to represent the design, relationships and variability you expect to see in your study, then test hypotheses which are both consistent with the data (to assess error rate) and inconsistent with the data (to assess power). If you have access to baseline data, or data on outcomes like the outcomes that you expect to gather, we encourage you to use them rather than the normally distributed random variables that we use here. For example, we expect power and validity will require larger studies if the outcomes are binary or very skewed.

```{r, error=FALSE, message=FALSE, warning=FALSE}
testH0andHtrue<-function(tau,totJ,n,rho) {
  ## Make data:

  ### Individual level data
  newDat<-ClusteredData(J=totJ,n=n,tau=tau,rho=rho)
  ### Cluster level data following Hansen and Bowers 2008
  Yj<-with(newDat,tapply(Y,J,sum))
  Zj<-with(newDat,tapply(Zi,J,fastmean))
  m0<-unique(table(newDat$J))

  ## Test Hypotheses
  ### Test null of no effects when no effects is false and tau is true.
  thelm<-lm(Y~Zi,data=newDat)
  thelmer<-lmer(Y~Zi+(1|J),data=newDat,control=lmerControl(optimizer='bobyqa'),REML=FALSE)

  lmci<-confint.HC(b=coef(thelm), thevcov=rcse(thelm,newDat$J)[[1]], thelm$df.residual)["Zi",]
  lmerci<-lme4:::confint.merMod(thelmer,parm="Zi",method="Wald")

  #### Zero should not be in this CI very often.
  zeroInCIlm<-  0 >= lmci[1] & 0 <= lmci[2]
  zeroInCIlmer<- 0 >= lmerci[1] & 0 <= lmerci[2]

  ### Do the Hansen and Bowers test and record not-rejections (equiv to "within CI")
  zeroNotRej<-hbtest(x=Yj,z=Zj,m0=m0)>=.05

  ### Test null of true taus (first attempt is use true, second is to make 0 true)
  ### Reassign village level treatment so that Y is indep of Z --- so true effect is 0

  newDat$Znew2 <- ifelse(newDat$J %in% sample(1:totJ, max(totJ)/2), 1, 0)
  Zj2<-with(newDat,tapply(Znew2,J,fastmean))

  thelmTrue<-lm(Y~Znew2,data=newDat)
  thelmerTrue<-lmer(Y~Znew2+(1|J),data=newDat,control=lmerControl(optimizer='bobyqa'),REML=FALSE)

  lmTrueci<-confint.HC(b=coef(thelmTrue), thevcov=rcse(thelmTrue,newDat$J)[[1]], thelmTrue$df.residual)["Znew2",]
  lmerTrueci<-lme4:::confint.merMod(thelmerTrue,parm="Znew2",method="Wald")

  trueInCIlm<-  0 >= lmTrueci[1] & 0 <= lmTrueci[2]
  trueInCIlmer<- 0 >= lmerTrueci[1] & 0 <= lmerTrueci[2]

  trueNotRej<-hbtest(x=Yj,z=Zj2,m0=m0)>=.05

  ## Consolidate results
  res<-array(NA,dim=c(2,3),dimnames=list(c("zero","true"),c("lm","lmer","hb")))
  res["zero","lm"]<-zeroInCIlm
  res["true","lm"]<-trueInCIlm
  res["zero","lmer"]<-zeroInCIlmer
  res["true","lmer"]<-trueInCIlmer
  res["zero","hb"]<-zeroNotRej
  res["true","hb"]<-trueNotRej
  return(res)
}
```

This next code chunk evaluates that preceding function 1000 times for each triple of $n$, $J$, and $τ$ (holding the ICC at .2).

```{r, error=FALSE, warning=FALSE, message=FALSE, eval=FALSE}
theJ<-c(4,8,10,20,30,50,80,100,200)
thenj<-c(4,8,10,20,30,50,80,100)
thetau<-c(.1,.25,.5,.75,1,1.5,2)
parms<-expand.grid(nj=thenj,J=theJ,tau=thetau)

## This next simulation takes time. To speed computation it uses all cores available (only works on Mac or Linux )
## The tryCatch() command is a more straightforward way to cache the results and not repeat this expensive computation.

library(parallel)
tryCatch(load("powerRes.rda"),error=function(e){
     powerRes<-mclapply(1:nrow(parms),function(i){
                message(paste(i,"/",nrow(parms)))
                replicate(1000,testH0andHtrue(tau=parms[i,"tau"],
                              totJ=parms[i,"J"],
                              n=parms[i,"nj"],
                              rho=.2))
         },mc.cores=detectCores())
     save(powerRes,file="powerRes.rda")
},finally=load("powerRes.rda"))
```

This chunk collects the results of the preceding simulation.

```{r, message=FALSE, warning=FALSE, error=FALSE, eval=FALSE}
resArr<-simplify2array(powerRes)
## str(resArr[,"lm",,])
powerArrLm<-apply(resArr[,"lm",,],c(1,3),mean)
powerArrLmer<-apply(resArr[,"lmer",,],c(1,3),mean)
powerArrHB<-apply(resArr[,"hb",,],c(1,3),mean)

results<-data.frame(parms,
            lm=t(powerArrLm),
            lmer=t(powerArrLmer),
            hb=t(powerArrHB))
results$lm.zero<-1-results$lm.zero
results$lmer.zero<-1-results$lmer.zero
results$hb.zero<-1-results$hb.zero
```

This next plot shows how the error rates change depending on the number of clusters. The vertical variability in the points plotted reflects the different effect sizes and cluster sizes. The dashed lines show the bounds that we would expect for simulation-to-simulation variability around the .95 line. We see that the OLS+RCSE and Multilevel model approaches do not produce valid tests in these scenarios until the the number of clusters is at least 30 if not 50.

![](https://raw.githubusercontent.com/egap/methods-guides/master/cluster-random/errorplots-1.png)

This next plot shows how power to detect an effect of $τ$ changes for the three approaches to statistical inference in cluster-randomized designs when (1) the number of clusters increases (within each plot on the x-axis) and the (2) effect size ($τ$) increases from .1 to 1 (different plots running from left to right on each row). The different lines on each plot show different cluster sizes. The cluster-sizes with the minimum and maximum power are labelled for each $J$. We compare the three approaches to statistical inference used here: the Cluster-level approach, the OLS+RCSE approach, and the Multilevel model approach. We see that as the effect size increases power increases and that increasing cluster size matters much less than increasing the number of clusters. All three approaches provide more or less the same power for a given collection of $J$, $n_j$, effect size and $ρ$. We do not show power for $J<30$ for the OLS+RCSE and the Multilevel approaches given the problems with error rates in those procedures that we discovered above.

![](https://raw.githubusercontent.com/egap/methods-guides/master/cluster-random/powerplots-1.png)

6 Randomization Checks in Clustered Designs
==
##6.1 Why check randomization in a randomized experiment?

There are two reasons to check randomization after treatment has been assigned. First, in small experiments, like those in which the cost of administering a treatment to another cluster is high, a background covariate of substantive importance may have an accidental and unlucky relationship with treatment assignment. Imagine that neighborhoods with highly educated households end up particularly prevalent in the treatment group during a study of voter turnout. A comparison of treated to untreated neighborhoods would then reflect both the differences in education between neighborhoods and as well as the treatment itself. If the covariate were available during the design stage of the study, one could use randomization tests to ensure balance at that stage by doing re-randomization (although blocking would make the analysis simpler — see Morgan and Rubin (2012)). Second, and more rarely, a randomization test can detect errors in the administration of treatment. Finding that the treatment was disproportionately allocated to highly educated neighborhoods might lead a researcher to take a second look at what happened in the field after treatment was assigned.

##6.2 How to check randomization?

Randomization checks in clustered designs follow the same form as the preceding discussion. A valid test for a treatment effect is a valid test for placebo or covariate balance. The only difference from our preceding discussion is that one uses a background covariate or baseline outcome — some variable putatively uninfluenced by the treatment — in place of the outcome itself. So, randomization tests with small numbers of clusters may be too quick to declare an experiment ill-randomized if the analyst is not aware of the methods of error-rate analysis that we described above.

One new problem does arise in the context of randomization tests. Often one has many covariates which could be used to detect unlucky imbalances or field problems with the randomization itself. And, if one uses hypothesis tests, then, of course, a valid test which encourages us to declare “imbalance” when $p<.05$ would do so falsely for one in every twenty variables tested. For this reason, we recommend using one-by-one testing as an exploratory tool and using omnibus tests (like the Hotelling T-test or an F-test or the Hansen and Bowers (2008) $d^2$ test), which can combine information across many dependent tests into one test statistic to make balance tests directly. However, these tests must account for the clustered nature of the design: a simple F-test without accounting for the clustered-design will likely mislead an analyst into declaring a design unbalanced and perhaps charging the field staff with a randomization failure.

Since cluster randomized experiments tend to have cluster-level covariates (say, village size, etc..), balance checks at the cluster level make sense and do not require explicit changes to account for clustered-assignment. Hansen and Bowers (2008) develop such a test and provide software to implement it. So, for example, if we had 10 covariates measured at the village level, and we had a large number of villages, we could assess an omnibus balance hypothesis using this design-based but large-sample tool.

Here we show only the omnibus test results. The one-by-one assessments that make up the omnibus test are also available in the `xb1` object. Here, the omnibus test tell us that we have little information against the null that these observations arose from a randomized study.

```{r, message=FALSE, error=FALSE, warning=FALSE, eval=FALSE}
library(RItools)
options(digits=3)

## Make a village level dataset
villageDat<-aggregate(pretendDat,by=list(village=pretendDat$J),mean)

## Generate 40 fake covariates
set.seed(12345)
villageDat[paste("x",1:40,sep="")]<-matrix(rnorm(nrow(villageDat)*40),ncol=40)
balfmla<-reformulate(paste("x",1:40,sep=""),response="Zi")
## Do a design-based, Large sample balance test.
xb1<-xBalance(balfmla, 
              strata=list(noblocks=NULL),
              data=villageDat,
              report=c("std.diffs","z.scores","adj.means",
                    "adj.mean.diffs", "chisquare.test","p.values"))
## print(xb1$results) ### to see the one-by-one tests

## Show the 10 of 40 one-by-one tests with the lowest p-values
kable(xb1$results[order(xb1$results[,"p",]),,][1:10,])

## Show the overall omnibus p-value
kable(xb1$overall)

## Plot the standardized differences
plot(xb1)
```

In this case, we cannot reject the omnibus hypotheses of balance even though, as we expected, we have a few covariates with falsely low p-values. One way to interpret this omnibus result is to say that such imbalances on a few covariates would not appreciably change any statistical inferences we make about treatment effects as long as these covariates did not strongly predict outcomes in the control group. Alternatively, we could say that any large experiment can tolerate chance imbalance on a few covariates (no more than roughly 5% if we are using $α=.05$ as our threshold to reject hypotheses).

References
==
Arceneaux, Kevin, and David W Nickerson. 2009. “Modeling Certainty with Clustered Data: A Comparison of Methods.” Political Analysis 17 (2). SPM-PMSAPSA: 177–90.

Barnard, J., C.E. Frangakis, J.L. Hill, and D.B. Rubin. 2003. “Principal Stratification Approach to Broken Randomized Experiments: A Case Study of School Choice Vouchers in New York City.” Journal of the American Statistical Association 98 (462). American Statistical Association: 299–324.

Bates, Douglas, Martin Maechler, Ben Bolker, and Steven Walker. 2014a. lme4: Linear Mixed-Effects Models Using Eigen and S4. http://CRAN.R-project.org/package=lme4.

Bates, Douglas, Martin Maechler, Benjamin M. Bolker, and Steven Walker. 2014b. “lme4: Linear Mixed-Effects Models Using Eigen and S4.” http://arxiv.org/abs/1406.5823.

Dunning, Thad. 2012. Natural Experiments in the Social Sciences: A Design-Based Approach. Cambridge University Press.

Gelman, A., and J. Hill. 2007. Data Analysis Using Regression and Multilevel/Hierarchical Models. Cambridge University Press.

Gelman, Andrew, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B Rubin. 2013. Bayesian Data Analysis. CRC press.

Gerber, A.S., and D.P. Green. 2012. Field Experiments: Design, Analysis, and Interpretation. WW Norton.

Green, D.P., and L. Vavreck. 2007. “Analysis of Cluster-Randomized Experiments: A Comparison of Alternative Estimation Approaches.” Political Analysis. SPM-PMSAPSA.

Hansen, B, and J Bowers. 2009. “Attributing Effects to a Cluster Randomized Get-Out-the-Vote Campaign.” Journal of the American Statistical Association 104 (487): 873—885. doi:10.1198/jasa.2009.ap06589.

Hansen, B., and J. Bowers. 2008. “Covariate Balance in Simple, Stratified and Clustered Comparative Studies.” Statistical Science 23 (2): 219–36.

Hong, Guanglei, and Stephen W Raudenbush. 2006. “Evaluating Kindergarten Retention Policy.” Journal of the American Statistical Association 101 (475).

Imai, K., G. King, and C. Nall. 2009. “The Essential Role of Pair Matching in Cluster-Randomized Experiments, with Application to the Mexican Universal Health Insurance Evaluation (with Discussion).” Statistical Science 24 (1): 29–72.

Kish, L. 1965. Survey Sampling. New York, NY: John Wiley; Sons.

Mathieu, John E, Herman Aguinis, Steven A Culpepper, and Gilad Chen. 2012a. “Understanding and Estimating the Power to Detect Cross-Level Interaction Effects in Multilevel Modeling.” Journal of Applied Psychology 97 (5). American Psychological Association: 951.

———. 2012b. “‘Understanding and Estimating the Power to Detect Cross-Level Interaction Effects in Multilevel Modeling’: Correction to Mathieu, Aguinis, Culpepper, and Chen (2012).” Journal of Applied Psychology. American Psychological Association.

Middleton, J.A. 2008. “Bias of the Regression Estimator for Experiments Using Clustered Random Assignment.” Statistics & Probability Letters 78 (16). Elsevier: 2654–59.

Middleton, Joel A, and Peter M Aronow. 2011. “Unbiased Estimation of the Average Treatment Effect in Cluster-Randomized Experiments.” Available at SSRN 1803849.

Morgan, Kari Lock, and Donald B Rubin. 2012. “Rerandomization to Improve Covariate Balance in Experiments.” The Annals of Statistics 40 (2). Institute of Mathematical Statistics: 1263–82.

R Core Team. 2014. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. http://www.R-project.org/.

Raudenbush, S.W. 1997. “Statistical analysis and optimal design for cluster randomized trials.” Psychological Methods 2 (2): 173–85.

Raudenbush, S.W., and X. Liu. 2000. “Statistical power and optimal design for multisite randomized trials.” Psychological Methods 5 (2): 199–213.

Raudenbush, Stephen W, Andres Martinez, and Jessaca Spybrook. 2007. “Strategies for Improving Precision in Group-Randomized Experiments.” Educational Evaluation and Policy Analysis 29 (1). Sage Publications: 5–29.

Small, D.S., T.R. Ten Have, and P.R. Rosenbaum. 2008. “Randomization Inference in a Group Randomized Trial of Treatments for Depression: Covariate Adjustment, Noncompliance, and Quantile Effects.” Journal of the American Statistical Association 103 (481). American Statistical Association: 271–79.

Spybrook, Jessaca, Howard Bloom, Richard Congdon, Carolyn Hill, Andres Martinez, Stephen Raudenbush, and APPLIES TO. 2011. “Optimal Design Plus Empirical Evidence: Documentation for the Optimal Design Software.” William T. Grant Foundation. Retrieved on November 5: 2012.

Stoker, L., and J. Bowers. 2002. “Erratum to ‘Designing Multi-Level Studies: Sampling Voters and Electoral Contexts’ [Electoral Studies 21 (2002) 235-267].” Electoral Studies 21 (3): 535–36. doi:10.1016/S0261-3794(02)00011-2.

Stoker, Laura, and Jake Bowers. 2002. “Designing Multi-Level Studies: Sampling Voters and Electoral Contexts.” Electoral Studies 21 (2): 235–67.